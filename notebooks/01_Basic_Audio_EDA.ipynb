{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Audio Exploratory Data Analysis\n",
    "\n",
    "This notebook demonstrates how to perform basic exploratory data analysis on audio files using the CTC-SpeechRefinement package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the project root to the Python path\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import Audio, display\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Import from the project\n",
    "from ctc_speech_refinement.core.preprocessing.audio import preprocess_audio\n",
    "from ctc_speech_refinement.core.eda.descriptive_stats import analyze_descriptive_stats\n",
    "from ctc_speech_refinement.core.eda.time_domain import analyze_time_domain\n",
    "from ctc_speech_refinement.core.eda.frequency_domain import analyze_frequency_domain\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Audio Data\n",
    "\n",
    "Let's load an audio file and examine its basic properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to an audio file\n",
    "audio_file = \"../data/test1/test1_01.wav\"  # Update this path to your audio file\n",
    "\n",
    "# Load the audio file\n",
    "audio_data, sample_rate = librosa.load(audio_file, sr=None)\n",
    "\n",
    "# Print basic information\n",
    "print(f\"Audio file: {audio_file}\")\n",
    "print(f\"Sample rate: {sample_rate} Hz\")\n",
    "print(f\"Duration: {len(audio_data) / sample_rate:.2f} seconds\")\n",
    "print(f\"Number of samples: {len(audio_data)}\")\n",
    "\n",
    "# Play the audio\n",
    "display(Audio(audio_data, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Waveform\n",
    "\n",
    "Let's visualize the waveform of the audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.waveshow(audio_data, sr=sample_rate)\n",
    "plt.title('Waveform')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics\n",
    "\n",
    "Let's compute and visualize some descriptive statistics of the audio data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute descriptive statistics\n",
    "stats = {\n",
    "    'Mean': np.mean(audio_data),\n",
    "    'Median': np.median(audio_data),\n",
    "    'Std Dev': np.std(audio_data),\n",
    "    'Min': np.min(audio_data),\n",
    "    'Max': np.max(audio_data),\n",
    "    'Range': np.max(audio_data) - np.min(audio_data),\n",
    "    'RMS': np.sqrt(np.mean(audio_data**2))\n",
    "}\n",
    "\n",
    "# Display statistics\n",
    "pd.DataFrame(stats, index=['Value']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot amplitude distribution\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.hist(audio_data, bins=100, alpha=0.7)\n",
    "plt.axvline(np.mean(audio_data), color='r', linestyle='dashed', linewidth=2, label=f'Mean: {np.mean(audio_data):.4f}')\n",
    "plt.axvline(np.median(audio_data), color='g', linestyle='dashed', linewidth=2, label=f'Median: {np.median(audio_data):.4f}')\n",
    "plt.title('Amplitude Distribution')\n",
    "plt.xlabel('Amplitude')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Domain Analysis\n",
    "\n",
    "Let's analyze the audio in the time domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute envelope\n",
    "def compute_envelope(audio_data, frame_length=2048, hop_length=512):\n",
    "    return np.array([max(audio_data[i:i+frame_length]) for i in range(0, len(audio_data), hop_length)])\n",
    "\n",
    "envelope = compute_envelope(audio_data)\n",
    "envelope_times = np.arange(len(envelope)) * (hop_length / sample_rate)\n",
    "\n",
    "# Plot envelope\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(envelope_times, envelope)\n",
    "plt.title('Audio Envelope')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute energy\n",
    "energy = librosa.feature.rms(y=audio_data, frame_length=2048, hop_length=512)[0]\n",
    "energy_times = librosa.times_like(energy, sr=sample_rate, hop_length=512)\n",
    "\n",
    "# Plot energy\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(energy_times, energy)\n",
    "plt.title('Energy (RMS)')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Energy')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute zero crossing rate\n",
    "zcr = librosa.feature.zero_crossing_rate(audio_data, frame_length=2048, hop_length=512)[0]\n",
    "zcr_times = librosa.times_like(zcr, sr=sample_rate, hop_length=512)\n",
    "\n",
    "# Plot zero crossing rate\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(zcr_times, zcr)\n",
    "plt.title('Zero Crossing Rate')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Rate')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Domain Analysis\n",
    "\n",
    "Let's analyze the audio in the frequency domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute spectrogram\n",
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(audio_data, n_fft=2048, hop_length=512)), ref=np.max)\n",
    "\n",
    "# Plot spectrogram\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(D, sr=sample_rate, x_axis='time', y_axis='log', hop_length=512)\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Spectrogram')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mel spectrogram\n",
    "mel_spec = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate, n_fft=2048, hop_length=512, n_mels=128)\n",
    "mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "# Plot mel spectrogram\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(mel_spec_db, sr=sample_rate, x_axis='time', y_axis='mel', hop_length=512)\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel Spectrogram')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute spectral centroid\n",
    "spectral_centroid = librosa.feature.spectral_centroid(y=audio_data, sr=sample_rate, n_fft=2048, hop_length=512)[0]\n",
    "spectral_centroid_times = librosa.times_like(spectral_centroid, sr=sample_rate, hop_length=512)\n",
    "\n",
    "# Plot spectral centroid\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.semilogy(spectral_centroid_times, spectral_centroid)\n",
    "plt.title('Spectral Centroid')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Frequency (Hz)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute spectral bandwidth\n",
    "spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio_data, sr=sample_rate, n_fft=2048, hop_length=512)[0]\n",
    "spectral_bandwidth_times = librosa.times_like(spectral_bandwidth, sr=sample_rate, hop_length=512)\n",
    "\n",
    "# Plot spectral bandwidth\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.semilogy(spectral_bandwidth_times, spectral_bandwidth)\n",
    "plt.title('Spectral Bandwidth')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Frequency (Hz)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitch and Timbre Analysis\n",
    "\n",
    "Let's analyze the pitch and timbre of the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pitch (fundamental frequency)\n",
    "pitches, magnitudes = librosa.piptrack(y=audio_data, sr=sample_rate, n_fft=2048, hop_length=512)\n",
    "pitch_times = librosa.times_like(pitches[0], sr=sample_rate, hop_length=512)\n",
    "\n",
    "# Extract the pitch with highest magnitude at each time\n",
    "pitch = []\n",
    "for t in range(pitches.shape[1]):\n",
    "    index = magnitudes[:, t].argmax()\n",
    "    pitch.append(pitches[index, t])\n",
    "\n",
    "# Plot pitch\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.semilogy(pitch_times, pitch)\n",
    "plt.title('Pitch (Fundamental Frequency)')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Frequency (Hz)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MFCCs\n",
    "mfccs = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=13, n_fft=2048, hop_length=512)\n",
    "\n",
    "# Plot MFCCs\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(mfccs, sr=sample_rate, x_axis='time', hop_length=512)\n",
    "plt.colorbar()\n",
    "plt.title('MFCCs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silence Detection\n",
    "\n",
    "Let's detect silent regions in the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect silent regions\n",
    "threshold = 0.01  # Adjust this threshold as needed\n",
    "silent_regions = librosa.effects.split(audio_data, top_db=20)\n",
    "\n",
    "# Convert to time\n",
    "silent_regions_time = [(start / sample_rate, end / sample_rate) for start, end in silent_regions]\n",
    "\n",
    "# Print silent regions\n",
    "print(\"Non-silent regions:\")\n",
    "for i, (start, end) in enumerate(silent_regions_time):\n",
    "    print(f\"Region {i+1}: {start:.2f}s - {end:.2f}s (duration: {end-start:.2f}s)\")\n",
    "\n",
    "# Plot waveform with non-silent regions highlighted\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.waveshow(audio_data, sr=sample_rate, alpha=0.5)\n",
    "\n",
    "# Highlight non-silent regions\n",
    "for start, end in silent_regions:\n",
    "    plt.axvspan(start / sample_rate, end / sample_rate, color='red', alpha=0.3)\n",
    "\n",
    "plt.title('Waveform with Non-Silent Regions Highlighted')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've performed a basic exploratory data analysis of an audio file. We've examined its waveform, computed descriptive statistics, and analyzed it in both the time and frequency domains. We've also looked at pitch and timbre features, and detected silent regions.\n",
    "\n",
    "This analysis provides a good starting point for understanding the characteristics of the audio data, which can be useful for preprocessing and feature extraction for speech recognition tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
