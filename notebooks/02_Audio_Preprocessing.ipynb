{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Preprocessing\n",
    "\n",
    "This notebook demonstrates how to preprocess audio files using the CTC-SpeechRefinement package. We'll explore various preprocessing techniques including normalization, silence removal, Voice Activity Detection (VAD), noise reduction, and frequency normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the project root to the Python path\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import Audio, display\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Import from the project\n",
    "from ctc_speech_refinement.core.preprocessing.audio import preprocess_audio, load_audio\n",
    "from ctc_speech_refinement.core.preprocessing.vad import apply_vad, energy_vad, zcr_vad\n",
    "from ctc_speech_refinement.core.preprocessing.noise_reduction import reduce_noise\n",
    "from ctc_speech_refinement.core.preprocessing.frequency_normalization import normalize_frequency\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Audio Data\n",
    "\n",
    "Let's load an audio file and examine its basic properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to an audio file\n",
    "audio_file = \"../data/speech2text/input/test1_01.wav\"  # Path to the audio file\n",
    "\n",
    "# Load the audio file using our package's function\n",
    "audio_data, sample_rate = load_audio(audio_file)\n",
    "\n",
    "# Print basic information\n",
    "print(f\"Audio file: {audio_file}\")\n",
    "print(f\"Sample rate: {sample_rate} Hz\")\n",
    "print(f\"Duration: {len(audio_data) / sample_rate:.2f} seconds\")\n",
    "print(f\"Number of samples: {len(audio_data)}\")\n",
    "\n",
    "# Play the audio\n",
    "display(Audio(audio_data, rate=sample_rate))\n",
    "\n",
    "# Plot the waveform\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.waveshow(audio_data, sr=sample_rate)\n",
    "plt.title('Original Waveform')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Normalization\n",
    "\n",
    "Let's normalize the audio data to have a maximum absolute amplitude of 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the audio with normalization only\n",
    "normalized_audio, _ = preprocess_audio(\n",
    "    audio_file, \n",
    "    normalize=True,\n",
    "    remove_silence_flag=False,\n",
    "    apply_vad_flag=False,\n",
    "    reduce_noise_flag=False,\n",
    "    normalize_frequency_flag=False\n",
    ")\n",
    "\n",
    "# Print information about the normalized audio\n",
    "print(f\"Original audio min: {np.min(audio_data):.6f}, max: {np.max(audio_data):.6f}\")\n",
    "print(f\"Normalized audio min: {np.min(normalized_audio):.6f}, max: {np.max(normalized_audio):.6f}\")\n",
    "\n",
    "# Play the normalized audio\n",
    "display(Audio(normalized_audio, rate=sample_rate))\n",
    "\n",
    "# Plot the original and normalized waveforms\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "librosa.display.waveshow(audio_data, sr=sample_rate, ax=ax1)\n",
    "ax1.set_title('Original Waveform')\n",
    "ax1.set_xlabel('Time (s)')\n",
    "ax1.set_ylabel('Amplitude')\n",
    "\n",
    "librosa.display.waveshow(normalized_audio, sr=sample_rate, ax=ax2)\n",
    "ax2.set_title('Normalized Waveform')\n",
    "ax2.set_xlabel('Time (s)')\n",
    "ax2.set_ylabel('Amplitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Silence Removal\n",
    "\n",
    "Let's remove silent regions from the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the audio with silence removal\n",
    "no_silence_audio, _ = preprocess_audio(\n",
    "    audio_file, \n",
    "    normalize=True,\n",
    "    remove_silence_flag=True,\n",
    "    apply_vad_flag=False,\n",
    "    reduce_noise_flag=False,\n",
    "    normalize_frequency_flag=False\n",
    ")\n",
    "\n",
    "# Print information about the audio with silence removed\n",
    "print(f\"Original audio duration: {len(audio_data) / sample_rate:.2f} seconds\")\n",
    "print(f\"Audio duration after silence removal: {len(no_silence_audio) / sample_rate:.2f} seconds\")\n",
    "print(f\"Reduction: {(1 - len(no_silence_audio) / len(audio_data)) * 100:.2f}%\")\n",
    "\n",
    "# Play the audio with silence removed\n",
    "display(Audio(no_silence_audio, rate=sample_rate))\n",
    "\n",
    "# Plot the original and silence-removed waveforms\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "librosa.display.waveshow(audio_data, sr=sample_rate, ax=ax1)\n",
    "ax1.set_title('Original Waveform')\n",
    "ax1.set_xlabel('Time (s)')\n",
    "ax1.set_ylabel('Amplitude')\n",
    "\n",
    "librosa.display.waveshow(no_silence_audio, sr=sample_rate, ax=ax2)\n",
    "ax2.set_title('Waveform with Silence Removed')\n",
    "ax2.set_xlabel('Time (s)')\n",
    "ax2.set_ylabel('Amplitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Voice Activity Detection (VAD)\n",
    "\n",
    "Let's apply Voice Activity Detection to keep only the speech segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the audio with VAD\n",
    "vad_audio, _ = preprocess_audio(\n",
    "    audio_file, \n",
    "    normalize=True,\n",
    "    remove_silence_flag=False,\n",
    "    apply_vad_flag=True,\n",
    "    vad_method=\"energy\",\n",
    "    reduce_noise_flag=False,\n",
    "    normalize_frequency_flag=False\n",
    ")\n",
    "\n",
    "# Print information about the audio after VAD\n",
    "print(f\"Original audio duration: {len(audio_data) / sample_rate:.2f} seconds\")\n",
    "print(f\"Audio duration after VAD: {len(vad_audio) / sample_rate:.2f} seconds\")\n",
    "print(f\"Reduction: {(1 - len(vad_audio) / len(audio_data)) * 100:.2f}%\")\n",
    "\n",
    "# Play the audio after VAD\n",
    "display(Audio(vad_audio, rate=sample_rate))\n",
    "\n",
    "# Plot the original and VAD-processed waveforms\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "librosa.display.waveshow(audio_data, sr=sample_rate, ax=ax1)\n",
    "ax1.set_title('Original Waveform')\n",
    "ax1.set_xlabel('Time (s)')\n",
    "ax1.set_ylabel('Amplitude')\n",
    "\n",
    "librosa.display.waveshow(vad_audio, sr=sample_rate, ax=ax2)\n",
    "ax2.set_title('Waveform after Voice Activity Detection')\n",
    "ax2.set_xlabel('Time (s)')\n",
    "ax2.set_ylabel('Amplitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Noise Reduction\n",
    "\n",
    "Let's apply noise reduction to the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the audio with noise reduction\n",
    "denoised_audio, _ = preprocess_audio(\n",
    "    audio_file, \n",
    "    normalize=True,\n",
    "    remove_silence_flag=False,\n",
    "    apply_vad_flag=False,\n",
    "    reduce_noise_flag=True,\n",
    "    noise_reduction_method=\"spectral_subtraction\",\n",
    "    normalize_frequency_flag=False\n",
    ")\n",
    "\n",
    "# Play the denoised audio\n",
    "display(Audio(denoised_audio, rate=sample_rate))\n",
    "\n",
    "# Plot the original and denoised waveforms\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "librosa.display.waveshow(audio_data, sr=sample_rate, ax=ax1)\n",
    "ax1.set_title('Original Waveform')\n",
    "ax1.set_xlabel('Time (s)')\n",
    "ax1.set_ylabel('Amplitude')\n",
    "\n",
    "librosa.display.waveshow(denoised_audio, sr=sample_rate, ax=ax2)\n",
    "ax2.set_title('Denoised Waveform')\n",
    "ax2.set_xlabel('Time (s)')\n",
    "ax2.set_ylabel('Amplitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Frequency Normalization\n",
    "\n",
    "Let's apply frequency normalization to the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the audio with frequency normalization\n",
    "freq_normalized_audio, _ = preprocess_audio(\n",
    "    audio_file, \n",
    "    normalize=True,\n",
    "    remove_silence_flag=False,\n",
    "    apply_vad_flag=False,\n",
    "    reduce_noise_flag=False,\n",
    "    normalize_frequency_flag=True,\n",
    "    frequency_normalization_method=\"bandpass\"\n",
    ")\n",
    "\n",
    "# Play the frequency-normalized audio\n",
    "display(Audio(freq_normalized_audio, rate=sample_rate))\n",
    "\n",
    "# Plot the original and frequency-normalized waveforms\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "librosa.display.waveshow(audio_data, sr=sample_rate, ax=ax1)\n",
    "ax1.set_title('Original Waveform')\n",
    "ax1.set_xlabel('Time (s)')\n",
    "ax1.set_ylabel('Amplitude')\n",
    "\n",
    "librosa.display.waveshow(freq_normalized_audio, sr=sample_rate, ax=ax2)\n",
    "ax2.set_title('Frequency-Normalized Waveform')\n",
    "ax2.set_xlabel('Time (s)')\n",
    "ax2.set_ylabel('Amplitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Combined Preprocessing\n",
    "\n",
    "Let's apply all preprocessing steps together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply all preprocessing steps\n",
    "fully_processed_audio, _ = preprocess_audio(\n",
    "    audio_file, \n",
    "    normalize=True,\n",
    "    remove_silence_flag=True,\n",
    "    apply_vad_flag=True,\n",
    "    vad_method=\"energy\",\n",
    "    reduce_noise_flag=True,\n",
    "    noise_reduction_method=\"spectral_subtraction\",\n",
    "    normalize_frequency_flag=True,\n",
    "    frequency_normalization_method=\"bandpass\"\n",
    ")\n",
    "\n",
    "# Print information about the fully processed audio\n",
    "print(f\"Original audio duration: {len(audio_data) / sample_rate:.2f} seconds\")\n",
    "print(f\"Fully processed audio duration: {len(fully_processed_audio) / sample_rate:.2f} seconds\")\n",
    "print(f\"Reduction: {(1 - len(fully_processed_audio) / len(audio_data)) * 100:.2f}%\")\n",
    "\n",
    "# Play the fully processed audio\n",
    "display(Audio(fully_processed_audio, rate=sample_rate))\n",
    "\n",
    "# Plot the original and fully processed waveforms\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "librosa.display.waveshow(audio_data, sr=sample_rate, ax=ax1)\n",
    "ax1.set_title('Original Waveform')\n",
    "ax1.set_xlabel('Time (s)')\n",
    "ax1.set_ylabel('Amplitude')\n",
    "\n",
    "librosa.display.waveshow(fully_processed_audio, sr=sample_rate, ax=ax2)\n",
    "ax2.set_title('Fully Processed Waveform')\n",
    "ax2.set_xlabel('Time (s)')\n",
    "ax2.set_ylabel('Amplitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated various audio preprocessing techniques using the CTC-SpeechRefinement package. We've explored normalization, silence removal, Voice Activity Detection (VAD), noise reduction, and frequency normalization. These preprocessing steps can significantly improve the quality of audio data for speech recognition tasks by removing noise, silence, and irrelevant frequency components."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
