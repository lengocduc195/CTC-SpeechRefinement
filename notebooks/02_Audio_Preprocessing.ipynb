{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Preprocessing\n",
    "\n",
    "This notebook demonstrates how to preprocess audio files using the CTC-SpeechRefinement package. We'll explore various preprocessing techniques including normalization, silence removal, Voice Activity Detection (VAD), noise reduction, and frequency normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the project root to the Python path\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import Audio, display\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Import from the project\n",
    "from ctc_speech_refinement.core.preprocessing.audio import preprocess_audio, load_audio\n",
    "from ctc_speech_refinement.core.preprocessing.vad import apply_vad, energy_vad, zcr_vad\n",
    "from ctc_speech_refinement.core.preprocessing.noise_reduction import reduce_noise\n",
    "from ctc_speech_refinement.core.preprocessing.frequency_normalization import normalize_frequency\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Audio Data\n",
    "\n",
    "Let's load an audio file and examine its basic properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to an audio file\n",
    "audio_file = \"../data/test1/test1_01.wav\"  # Update this path to your audio file\n",
    "\n",
    "# Load the audio file\n",
    "audio_data, sample_rate = load_audio(audio_file)\n",
    "\n",
    "# Print basic information\n",
    "print(f\"Audio file: {audio_file}\")\n",
    "print(f\"Sample rate: {sample_rate} Hz\")\n",
    "print(f\"Duration: {len(audio_data) / sample_rate:.2f} seconds\")\n",
    "print(f\"Number of samples: {len(audio_data)}\")\n",
    "\n",
    "# Play the audio\n",
    "display(Audio(audio_data, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Original Waveform\n",
    "\n",
    "Let's visualize the waveform of the original audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.waveshow(audio_data, sr=sample_rate)\n",
    "plt.title('Original Waveform')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Amplitude Normalization\n",
    "\n",
    "Let's normalize the amplitude of the audio data to have zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to normalize audio data\n",
    "def normalize_audio(audio_data):\n",
    "    mean = np.mean(audio_data)\n",
    "    std = np.std(audio_data)\n",
    "    if std > 0:\n",
    "        normalized_audio = (audio_data - mean) / std\n",
    "    else:\n",
    "        normalized_audio = audio_data - mean\n",
    "    return normalized_audio\n",
    "\n",
    "# Normalize the audio data\n",
    "normalized_audio = normalize_audio(audio_data)\n",
    "\n",
    "# Print statistics before and after normalization\n",
    "print(\"Before normalization:\")\n",
    "print(f\"Mean: {np.mean(audio_data):.6f}\")\n",
    "print(f\"Std Dev: {np.std(audio_data):.6f}\")\n",
    "print(f\"Min: {np.min(audio_data):.6f}\")\n",
    "print(f\"Max: {np.max(audio_data):.6f}\")\n",
    "print(\"\\nAfter normalization:\")\n",
    "print(f\"Mean: {np.mean(normalized_audio):.6f}\")\n",
    "print(f\"Std Dev: {np.std(normalized_audio):.6f}\")\n",
    "print(f\"Min: {np.min(normalized_audio):.6f}\")\n",
    "print(f\"Max: {np.max(normalized_audio):.6f}\")\n",
    "\n",
    "# Play the normalized audio\n",
    "display(Audio(normalized_audio, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original and normalized waveforms\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "librosa.display.waveshow(audio_data, sr=sample_rate)\n",
    "plt.title('Original Waveform')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "librosa.display.waveshow(normalized_audio, sr=sample_rate)\n",
    "plt.title('Normalized Waveform')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Silence Removal\n",
    "\n",
    "Let's remove silent regions from the audio data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove silence\n",
    "def remove_silence(audio_data, sample_rate, top_db=60, frame_length=2048, hop_length=512):\n",
    "    non_silent_intervals = librosa.effects.split(audio_data, top_db=top_db, frame_length=frame_length, hop_length=hop_length)\n",
    "    \n",
    "    if len(non_silent_intervals) == 0:\n",
    "        print(\"No non-silent intervals found\")\n",
    "        return audio_data\n",
    "    \n",
    "    non_silent_audio = []\n",
    "    for interval in non_silent_intervals:\n",
    "        start, end = interval\n",
    "        non_silent_audio.extend(audio_data[start:end])\n",
    "    \n",
    "    return np.array(non_silent_audio)\n",
    "\n",
    "# Remove silence from the audio data\n",
    "audio_without_silence = remove_silence(audio_data, sample_rate)\n",
    "\n",
    "# Print duration before and after silence removal\n",
    "print(f\"Duration before silence removal: {len(audio_data) / sample_rate:.2f} seconds\")\n",
    "print(f\"Duration after silence removal: {len(audio_without_silence) / sample_rate:.2f} seconds\")\n",
    "print(f\"Reduction: {(1 - len(audio_without_silence) / len(audio_data)) * 100:.2f}%\")\n",
    "\n",
    "# Play the audio without silence\n",
    "display(Audio(audio_without_silence, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original and silence-removed waveforms\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "librosa.display.waveshow(audio_data, sr=sample_rate)\n",
    "plt.title('Original Waveform')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "librosa.display.waveshow(audio_without_silence, sr=sample_rate)\n",
    "plt.title('Waveform without Silence')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Voice Activity Detection (VAD)\n",
    "\n",
    "Let's apply Voice Activity Detection to extract speech segments from the audio data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply energy-based VAD\n",
    "speech_regions_energy = energy_vad(audio_data, sample_rate)\n",
    "\n",
    "# Print speech regions\n",
    "print(\"Speech regions (energy-based VAD):\")\n",
    "for i, (start, end) in enumerate(speech_regions_energy):\n",
    "    print(f\"Region {i+1}: {start:.2f}s - {end:.2f}s (duration: {end-start:.2f}s)\")\n",
    "\n",
    "# Apply VAD to extract speech segments\n",
    "speech_audio_energy = apply_vad(audio_data, sample_rate, method=\"energy\")\n",
    "\n",
    "# Print duration before and after VAD\n",
    "print(f\"\\nDuration before VAD: {len(audio_data) / sample_rate:.2f} seconds\")\n",
    "print(f\"Duration after VAD: {len(speech_audio_energy) / sample_rate:.2f} seconds\")\n",
    "print(f\"Reduction: {(1 - len(speech_audio_energy) / len(audio_data)) * 100:.2f}%\")\n",
    "\n",
    "# Play the speech audio\n",
    "display(Audio(speech_audio_energy, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply ZCR-based VAD\n",
    "speech_regions_zcr = zcr_vad(audio_data, sample_rate)\n",
    "\n",
    "# Print speech regions\n",
    "print(\"Speech regions (ZCR-based VAD):\")\n",
    "for i, (start, end) in enumerate(speech_regions_zcr):\n",
    "    print(f\"Region {i+1}: {start:.2f}s - {end:.2f}s (duration: {end-start:.2f}s)\")\n",
    "\n",
    "# Apply VAD to extract speech segments\n",
    "speech_audio_zcr = apply_vad(audio_data, sample_rate, method=\"zcr\")\n",
    "\n",
    "# Print duration before and after VAD\n",
    "print(f\"\\nDuration before VAD: {len(audio_data) / sample_rate:.2f} seconds\")\n",
    "print(f\"Duration after VAD: {len(speech_audio_zcr) / sample_rate:.2f} seconds\")\n",
    "print(f\"Reduction: {(1 - len(speech_audio_zcr) / len(audio_data)) * 100:.2f}%\")\n",
    "\n",
    "# Play the speech audio\n",
    "display(Audio(speech_audio_zcr, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original and VAD-processed waveforms\n",
    "plt.figure(figsize=(14, 12))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "librosa.display.waveshow(audio_data, sr=sample_rate)\n",
    "plt.title('Original Waveform')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "librosa.display.waveshow(speech_audio_energy, sr=sample_rate)\n",
    "plt.title('Waveform after Energy-based VAD')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "librosa.display.waveshow(speech_audio_zcr, sr=sample_rate)\n",
    "plt.title('Waveform after ZCR-based VAD')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Noise Reduction\n",
    "\n",
    "Let's apply noise reduction to the audio data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply spectral subtraction for noise reduction\n",
    "denoised_audio_spectral = reduce_noise(audio_data, sample_rate, method=\"spectral_subtraction\")\n",
    "\n",
    "# Play the denoised audio\n",
    "print(\"Denoised audio (spectral subtraction):\")\n",
    "display(Audio(denoised_audio_spectral, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Wiener filter for noise reduction\n",
    "denoised_audio_wiener = reduce_noise(audio_data, sample_rate, method=\"wiener\")\n",
    "\n",
    "# Play the denoised audio\n",
    "print(\"Denoised audio (Wiener filter):\")\n",
    "display(Audio(denoised_audio_wiener, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply median filter for noise reduction\n",
    "denoised_audio_median = reduce_noise(audio_data, sample_rate, method=\"median\")\n",
    "\n",
    "# Play the denoised audio\n",
    "print(\"Denoised audio (median filter):\")\n",
    "display(Audio(denoised_audio_median, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original and denoised waveforms\n",
    "plt.figure(figsize=(14, 12))\n",
    "\n",
    "plt.subplot(4, 1, 1)\n",
    "librosa.display.waveshow(audio_data, sr=sample_rate)\n",
    "plt.title('Original Waveform')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "librosa.display.waveshow(denoised_audio_spectral, sr=sample_rate)\n",
    "plt.title('Waveform after Spectral Subtraction')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "librosa.display.waveshow(denoised_audio_wiener, sr=sample_rate)\n",
    "plt.title('Waveform after Wiener Filter')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "librosa.display.waveshow(denoised_audio_median, sr=sample_rate)\n",
    "plt.title('Waveform after Median Filter')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Frequency Normalization\n",
    "\n",
    "Let's apply frequency normalization to the audio data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply bandpass filter for frequency normalization\n",
    "normalized_audio_bandpass = normalize_frequency(audio_data, sample_rate, method=\"bandpass\")\n",
    "\n",
    "# Play the frequency-normalized audio\n",
    "print(\"Frequency-normalized audio (bandpass filter):\")\n",
    "display(Audio(normalized_audio_bandpass, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply pre-emphasis for frequency normalization\n",
    "normalized_audio_preemphasis = normalize_frequency(audio_data, sample_rate, method=\"preemphasis\")\n",
    "\n",
    "# Play the frequency-normalized audio\n",
    "print(\"Frequency-normalized audio (pre-emphasis):\")\n",
    "display(Audio(normalized_audio_preemphasis, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply spectral equalization for frequency normalization\n",
    "normalized_audio_equalize = normalize_frequency(audio_data, sample_rate, method=\"equalize\")\n",
    "\n",
    "# Play the frequency-normalized audio\n",
    "print(\"Frequency-normalized audio (spectral equalization):\")\n",
    "display(Audio(normalized_audio_equalize, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply combined methods for frequency normalization\n",
    "normalized_audio_combined = normalize_frequency(audio_data, sample_rate, method=\"combined\")\n",
    "\n",
    "# Play the frequency-normalized audio\n",
    "print(\"Frequency-normalized audio (combined methods):\")\n",
    "display(Audio(normalized_audio_combined, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original and frequency-normalized waveforms\n",
    "plt.figure(figsize=(14, 15))\n",
    "\n",
    "plt.subplot(5, 1, 1)\n",
    "librosa.display.waveshow(audio_data, sr=sample_rate)\n",
    "plt.title('Original Waveform')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "plt.subplot(5, 1, 2)\n",
    "librosa.display.waveshow(normalized_audio_bandpass, sr=sample_rate)\n",
    "plt.title('Waveform after Bandpass Filter')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "plt.subplot(5, 1, 3)\n",
    "librosa.display.waveshow(normalized_audio_preemphasis, sr=sample_rate)\n",
    "plt.title('Waveform after Pre-emphasis')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "plt.subplot(5, 1, 4)\n",
    "librosa.display.waveshow(normalized_audio_equalize, sr=sample_rate)\n",
    "plt.title('Waveform after Spectral Equalization')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "plt.subplot(5, 1, 5)\n",
    "librosa.display.waveshow(normalized_audio_combined, sr=sample_rate)\n",
    "plt.title('Waveform after Combined Methods')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Combined Preprocessing Pipeline\n",
    "\n",
    "Let's apply a combined preprocessing pipeline to the audio data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the full preprocessing pipeline\n",
    "preprocessed_audio, preprocessed_sample_rate = preprocess_audio(\n",
    "    audio_file,\n",
    "    normalize=True,\n",
    "    remove_silence_flag=True,\n",
    "    apply_vad_flag=True,\n",
    "    vad_method=\"energy\",\n",
    "    reduce_noise_flag=True,\n",
    "    noise_reduction_method=\"spectral_subtraction\",\n",
    "    normalize_frequency_flag=True,\n",
    "    frequency_normalization_method=\"bandpass\"\n",
    ")\n",
    "\n",
    "# Print duration before and after preprocessing\n",
    "print(f\"Duration before preprocessing: {len(audio_data) / sample_rate:.2f} seconds\")\n",
    "print(f\"Duration after preprocessing: {len(preprocessed_audio) / preprocessed_sample_rate:.2f} seconds\")\n",
    "print(f\"Reduction: {(1 - len(preprocessed_audio) / len(audio_data)) * 100:.2f}%\")\n",
    "\n",
    "# Play the preprocessed audio\n",
    "display(Audio(preprocessed_audio, rate=preprocessed_sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original and fully preprocessed waveforms\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "librosa.display.waveshow(audio_data, sr=sample_rate)\n",
    "plt.title('Original Waveform')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "librosa.display.waveshow(preprocessed_audio, sr=preprocessed_sample_rate)\n",
    "plt.title('Fully Preprocessed Waveform')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Spectrograms Before and After Preprocessing\n",
    "\n",
    "Let's compare the spectrograms of the original and preprocessed audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute spectrograms\n",
    "D_original = librosa.amplitude_to_db(np.abs(librosa.stft(audio_data, n_fft=2048, hop_length=512)), ref=np.max)\n",
    "D_preprocessed = librosa.amplitude_to_db(np.abs(librosa.stft(preprocessed_audio, n_fft=2048, hop_length=512)), ref=np.max)\n",
    "\n",
    "# Plot spectrograms\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "librosa.display.specshow(D_original, sr=sample_rate, x_axis='time', y_axis='log', hop_length=512)\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Original Spectrogram')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "librosa.display.specshow(D_preprocessed, sr=preprocessed_sample_rate, x_axis='time', y_axis='log', hop_length=512)\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Preprocessed Spectrogram')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored various audio preprocessing techniques including amplitude normalization, silence removal, Voice Activity Detection (VAD), noise reduction, and frequency normalization. We've also applied a combined preprocessing pipeline to the audio data.\n",
    "\n",
    "These preprocessing techniques can significantly improve the quality of audio data for speech recognition tasks by removing noise, enhancing speech segments, and normalizing the audio signal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
