{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Audio Visualization Techniques\n",
    "\n",
    "This notebook demonstrates advanced techniques for visualizing audio data using the CTC-SpeechRefinement package. We'll explore various visualization methods that can provide insights into different aspects of audio signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the project root to the Python path\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import Audio, display\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from scipy import signal\n",
    "\n",
    "# Import from the project\n",
    "from ctc_speech_refinement.core.preprocessing.audio import preprocess_audio\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Audio Data\n",
    "\n",
    "Let's load an audio file and examine its basic properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to an audio file\n",
    "audio_file = \"../data/test1/test1_01.wav\"  # Update this path to your audio file\n",
    "\n",
    "# Load the audio file\n",
    "audio_data, sample_rate = librosa.load(audio_file, sr=None)\n",
    "\n",
    "# Print basic information\n",
    "print(f\"Audio file: {audio_file}\")\n",
    "print(f\"Sample rate: {sample_rate} Hz\")\n",
    "print(f\"Duration: {len(audio_data) / sample_rate:.2f} seconds\")\n",
    "print(f\"Number of samples: {len(audio_data)}\")\n",
    "\n",
    "# Play the audio\n",
    "display(Audio(audio_data, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Enhanced Waveform Visualization\n",
    "\n",
    "Let's create enhanced visualizations of the audio waveform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic waveform plot\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.waveshow(audio_data, sr=sample_rate, alpha=0.6)\n",
    "plt.title('Audio Waveform')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced waveform with envelope\n",
    "def compute_envelope(audio_data, frame_length=2048, hop_length=512):\n",
    "    return np.array([max(audio_data[i:i+frame_length]) for i in range(0, len(audio_data), hop_length)])\n",
    "\n",
    "# Compute envelope\n",
    "frame_length = 2048\n",
    "hop_length = 512\n",
    "envelope = compute_envelope(audio_data, frame_length, hop_length)\n",
    "envelope_times = np.arange(len(envelope)) * (hop_length / sample_rate)\n",
    "\n",
    "# Plot waveform with envelope\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.waveshow(audio_data, sr=sample_rate, alpha=0.6)\n",
    "plt.plot(envelope_times, envelope, color='red', linewidth=2, label='Envelope')\n",
    "plt.plot(envelope_times, -envelope, color='red', linewidth=2)\n",
    "plt.title('Audio Waveform with Envelope')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waveform with color-coded amplitude\n",
    "plt.figure(figsize=(14, 5))\n",
    "times = np.arange(len(audio_data)) / sample_rate\n",
    "plt.scatter(times, audio_data, c=np.abs(audio_data), cmap='viridis', s=1, alpha=0.5)\n",
    "plt.colorbar(label='Absolute Amplitude')\n",
    "plt.title('Audio Waveform with Color-Coded Amplitude')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Enhanced Spectrogram Visualization\n",
    "\n",
    "Let's create enhanced visualizations of the audio spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute STFT\n",
    "n_fft = 2048\n",
    "hop_length = 512\n",
    "stft = librosa.stft(audio_data, n_fft=n_fft, hop_length=hop_length)\n",
    "stft_magnitude = np.abs(stft)\n",
    "stft_db = librosa.amplitude_to_db(stft_magnitude, ref=np.max)\n",
    "\n",
    "# Basic spectrogram\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(stft_db, sr=sample_rate, x_axis='time', y_axis='log', hop_length=hop_length)\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Spectrogram')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced spectrogram with custom colormap\n",
    "# Create a custom colormap\n",
    "colors = [(0, 0, 0), (0, 0, 1), (0, 1, 1), (1, 1, 0), (1, 0, 0)]\n",
    "custom_cmap = LinearSegmentedColormap.from_list('custom_cmap', colors)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(stft_db, sr=sample_rate, x_axis='time', y_axis='log', hop_length=hop_length, cmap=custom_cmap)\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Enhanced Spectrogram with Custom Colormap')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectrogram with waveform overlay\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot spectrogram\n",
    "ax1 = plt.subplot(2, 1, 1)\n",
    "librosa.display.specshow(stft_db, sr=sample_rate, x_axis='time', y_axis='log', hop_length=hop_length)\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Spectrogram')\n",
    "\n",
    "# Plot waveform\n",
    "ax2 = plt.subplot(2, 1, 2, sharex=ax1)\n",
    "librosa.display.waveshow(audio_data, sr=sample_rate)\n",
    "plt.title('Waveform')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 3D Visualization\n",
    "\n",
    "Let's create 3D visualizations of the audio data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D spectrogram using matplotlib\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Prepare data for 3D plot\n",
    "times = librosa.times_like(stft[0], sr=sample_rate, hop_length=hop_length)\n",
    "freqs = librosa.fft_frequencies(sr=sample_rate, n_fft=n_fft)\n",
    "\n",
    "# Downsample for better visualization\n",
    "downsample_factor_time = 4\n",
    "downsample_factor_freq = 4\n",
    "times_downsampled = times[::downsample_factor_time]\n",
    "freqs_downsampled = freqs[::downsample_factor_freq]\n",
    "stft_db_downsampled = stft_db[::downsample_factor_freq, ::downsample_factor_time]\n",
    "\n",
    "# Create meshgrid\n",
    "time_grid, freq_grid = np.meshgrid(times_downsampled, freqs_downsampled)\n",
    "\n",
    "# Plot 3D surface\n",
    "surf = ax.plot_surface(time_grid, freq_grid, stft_db_downsampled, cmap='viridis', alpha=0.8)\n",
    "\n",
    "# Add colorbar\n",
    "fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5, label='Magnitude (dB)')\n",
    "\n",
    "# Set labels\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('Frequency (Hz)')\n",
    "ax.set_zlabel('Magnitude (dB)')\n",
    "ax.set_title('3D Spectrogram')\n",
    "\n",
    "# Set frequency axis to log scale\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim(20, sample_rate/2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D spectrogram using plotly for interactive visualization\n",
    "# Prepare data for 3D plot\n",
    "times = librosa.times_like(stft[0], sr=sample_rate, hop_length=hop_length)\n",
    "freqs = librosa.fft_frequencies(sr=sample_rate, n_fft=n_fft)\n",
    "\n",
    "# Downsample for better visualization\n",
    "downsample_factor_time = 4\n",
    "downsample_factor_freq = 4\n",
    "times_downsampled = times[::downsample_factor_time]\n",
    "freqs_downsampled = freqs[::downsample_factor_freq]\n",
    "stft_db_downsampled = stft_db[::downsample_factor_freq, ::downsample_factor_time]\n",
    "\n",
    "# Create meshgrid\n",
    "time_grid, freq_grid = np.meshgrid(times_downsampled, freqs_downsampled)\n",
    "\n",
    "# Create 3D surface plot\n",
    "fig = go.Figure(data=[go.Surface(z=stft_db_downsampled, x=time_grid, y=freq_grid, colorscale='Viridis')])\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Interactive 3D Spectrogram',\n",
    "    scene=dict(\n",
    "        xaxis_title='Time (s)',\n",
    "        yaxis_title='Frequency (Hz)',\n",
    "        zaxis_title='Magnitude (dB)',\n",
    "        yaxis=dict(type='log', range=[np.log10(20), np.log10(sample_rate/2)])\n",
    "    ),\n",
    "    width=900,\n",
    "    height=700\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Time-Frequency Analysis Visualization\n",
    "\n",
    "Let's create visualizations for time-frequency analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute CQT (Constant-Q Transform)\n",
    "cqt = librosa.cqt(audio_data, sr=sample_rate, hop_length=hop_length)\n",
    "cqt_db = librosa.amplitude_to_db(np.abs(cqt), ref=np.max)\n",
    "\n",
    "# Plot CQT\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(cqt_db, sr=sample_rate, x_axis='time', y_axis='cqt_note', hop_length=hop_length)\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Constant-Q Transform (CQT)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute wavelet transform\n",
    "# Using continuous wavelet transform from scipy\n",
    "widths = np.arange(1, 31)\n",
    "cwtmatr = signal.cwt(audio_data[:sample_rate], signal.ricker, widths)\n",
    "\n",
    "# Plot wavelet transform\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.imshow(cwtmatr, extent=[0, 1, 1, 31], cmap='viridis', aspect='auto', vmax=abs(cwtmatr).max(), vmin=-abs(cwtmatr).max())\n",
    "plt.colorbar(label='Amplitude')\n",
    "plt.title('Wavelet Transform (First Second of Audio)')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Scale')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Visualization\n",
    "\n",
    "Let's create visualizations for various audio features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MFCCs\n",
    "n_mfcc = 13\n",
    "mfccs = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "# Plot MFCCs\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(mfccs, sr=sample_rate, x_axis='time', hop_length=hop_length)\n",
    "plt.colorbar()\n",
    "plt.title('MFCCs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute chroma features\n",
    "chroma = librosa.feature.chroma_stft(y=audio_data, sr=sample_rate, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "# Plot chroma features\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(chroma, sr=sample_rate, x_axis='time', y_axis='chroma', hop_length=hop_length)\n",
    "plt.colorbar()\n",
    "plt.title('Chromagram')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute spectral features\n",
    "spectral_centroid = librosa.feature.spectral_centroid(y=audio_data, sr=sample_rate, n_fft=n_fft, hop_length=hop_length)[0]\n",
    "spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio_data, sr=sample_rate, n_fft=n_fft, hop_length=hop_length)[0]\n",
    "spectral_contrast = librosa.feature.spectral_contrast(y=audio_data, sr=sample_rate, n_fft=n_fft, hop_length=hop_length)\n",
    "spectral_flatness = librosa.feature.spectral_flatness(y=audio_data, n_fft=n_fft, hop_length=hop_length)[0]\n",
    "spectral_rolloff = librosa.feature.spectral_rolloff(y=audio_data, sr=sample_rate, n_fft=n_fft, hop_length=hop_length)[0]\n",
    "\n",
    "# Create time axis\n",
    "feature_times = librosa.times_like(spectral_centroid, sr=sample_rate, hop_length=hop_length)\n",
    "\n",
    "# Plot spectral features\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.semilogy(feature_times, spectral_centroid, label='Centroid')\n",
    "plt.semilogy(feature_times, spectral_rolloff, label='Rolloff', alpha=0.7)\n",
    "plt.title('Spectral Centroid and Rolloff')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Frequency (Hz)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.semilogy(feature_times, spectral_bandwidth, label='Bandwidth')\n",
    "plt.title('Spectral Bandwidth')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Frequency (Hz)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(feature_times, spectral_flatness, label='Flatness')\n",
    "plt.title('Spectral Flatness')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Flatness')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Combined Visualization\n",
    "\n",
    "Let's create a combined visualization that shows multiple aspects of the audio signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive visualization\n",
    "plt.figure(figsize=(14, 15))\n",
    "\n",
    "# Plot waveform\n",
    "ax1 = plt.subplot(5, 1, 1)\n",
    "librosa.display.waveshow(audio_data, sr=sample_rate)\n",
    "plt.title('Waveform')\n",
    "plt.xlabel('')\n",
    "\n",
    "# Plot spectrogram\n",
    "ax2 = plt.subplot(5, 1, 2, sharex=ax1)\n",
    "librosa.display.specshow(stft_db, sr=sample_rate, x_axis='time', y_axis='log', hop_length=hop_length)\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Spectrogram')\n",
    "plt.xlabel('')\n",
    "\n",
    "# Plot MFCCs\n",
    "ax3 = plt.subplot(5, 1, 3, sharex=ax1)\n",
    "librosa.display.specshow(mfccs, sr=sample_rate, x_axis='time', hop_length=hop_length)\n",
    "plt.colorbar()\n",
    "plt.title('MFCCs')\n",
    "plt.xlabel('')\n",
    "\n",
    "# Plot chroma\n",
    "ax4 = plt.subplot(5, 1, 4, sharex=ax1)\n",
    "librosa.display.specshow(chroma, sr=sample_rate, x_axis='time', y_axis='chroma', hop_length=hop_length)\n",
    "plt.colorbar()\n",
    "plt.title('Chromagram')\n",
    "plt.xlabel('')\n",
    "\n",
    "# Plot spectral features\n",
    "ax5 = plt.subplot(5, 1, 5, sharex=ax1)\n",
    "plt.semilogy(feature_times, spectral_centroid, label='Centroid')\n",
    "plt.semilogy(feature_times, spectral_rolloff, label='Rolloff', alpha=0.7)\n",
    "plt.title('Spectral Features')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored various advanced techniques for visualizing audio data. We've created enhanced waveform visualizations, enhanced spectrogram visualizations, 3D visualizations, time-frequency analysis visualizations, feature visualizations, and combined visualizations.\n",
    "\n",
    "These visualization techniques provide valuable insights into different aspects of audio signals, which can be useful for understanding the characteristics of speech signals and for feature extraction in speech recognition tasks. By using these techniques, we can gain a deeper understanding of the audio data and make more informed decisions about preprocessing and feature extraction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
