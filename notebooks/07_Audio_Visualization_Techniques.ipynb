{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Visualization Techniques\n",
    "\n",
    "This notebook demonstrates various techniques for visualizing audio data using the CTC-SpeechRefinement package. We'll explore different ways to represent audio signals visually, which can help in understanding their characteristics and patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the project root to the Python path\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import Audio, display, HTML\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Import from the project\n",
    "from ctc_speech_refinement.core.preprocessing.audio import load_audio\n",
    "from ctc_speech_refinement.core.eda.descriptive_stats import analyze_descriptive_stats\n",
    "from ctc_speech_refinement.core.eda.time_domain import analyze_time_domain\n",
    "from ctc_speech_refinement.core.eda.frequency_domain import analyze_frequency_domain\n",
    "from ctc_speech_refinement.core.eda.pitch_timbre import analyze_pitch_timbre\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Audio Data\n",
    "\n",
    "Let's load an audio file and examine its basic properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to an audio file\n",
    "audio_file = \"../data/test1/test1_01.wav\"  # Update this path to your audio file\n",
    "\n",
    "# Load the audio file using our package's function\n",
    "audio_data, sample_rate = load_audio(audio_file)\n",
    "\n",
    "# Print basic information\n",
    "print(f\"Audio file: {audio_file}\")\n",
    "print(f\"Sample rate: {sample_rate} Hz\")\n",
    "print(f\"Duration: {len(audio_data) / sample_rate:.2f} seconds\")\n",
    "print(f\"Number of samples: {len(audio_data)}\")\n",
    "\n",
    "# Play the audio\n",
    "display(Audio(audio_data, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Waveform Visualization\n",
    "\n",
    "Let's start with the basic waveform visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time array\n",
    "time = np.linspace(0, len(audio_data) / sample_rate, len(audio_data))\n",
    "\n",
    "# Plot waveform\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(time, audio_data, color='blue', alpha=0.7)\n",
    "plt.title('Audio Waveform')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Enhanced Waveform Visualization\n",
    "\n",
    "Let's enhance the waveform visualization with additional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute RMS energy\n",
    "frame_length = 2048\n",
    "hop_length = 512\n",
    "rms = librosa.feature.rms(y=audio_data, frame_length=frame_length, hop_length=hop_length)[0]\n",
    "rms_times = librosa.times_like(rms, sr=sample_rate, hop_length=hop_length)\n",
    "\n",
    "# Plot enhanced waveform\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot waveform\n",
    "plt.plot(time, audio_data, color='blue', alpha=0.5, label='Waveform')\n",
    "\n",
    "# Plot RMS energy\n",
    "plt.plot(rms_times, rms, color='red', linewidth=2, label='RMS Energy')\n",
    "\n",
    "# Add horizontal lines for mean and max amplitude\n",
    "plt.axhline(y=np.mean(audio_data), color='green', linestyle='--', linewidth=1, label=f'Mean: {np.mean(audio_data):.4f}')\n",
    "plt.axhline(y=np.max(np.abs(audio_data)), color='orange', linestyle='--', linewidth=1, label=f'Max: {np.max(np.abs(audio_data)):.4f}')\n",
    "\n",
    "plt.title('Enhanced Audio Waveform')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Spectrogram Visualization\n",
    "\n",
    "Let's visualize the spectrogram of the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute spectrogram\n",
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(audio_data, n_fft=2048, hop_length=512)), ref=np.max)\n",
    "\n",
    "# Plot spectrogram\n",
    "plt.figure(figsize=(14, 7))\n",
    "librosa.display.specshow(D, sr=sample_rate, x_axis='time', y_axis='log', hop_length=512)\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Spectrogram')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mel Spectrogram Visualization\n",
    "\n",
    "Let's visualize the Mel spectrogram of the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mel spectrogram\n",
    "mel_spec = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate, n_fft=2048, hop_length=512, n_mels=128)\n",
    "mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "# Plot mel spectrogram\n",
    "plt.figure(figsize=(14, 7))\n",
    "librosa.display.specshow(mel_spec_db, sr=sample_rate, x_axis='time', y_axis='mel', hop_length=512)\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel Spectrogram')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chromagram Visualization\n",
    "\n",
    "Let's visualize the chromagram of the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute chromagram\n",
    "chroma = librosa.feature.chroma_stft(y=audio_data, sr=sample_rate, n_fft=2048, hop_length=512)\n",
    "\n",
    "# Plot chromagram\n",
    "plt.figure(figsize=(14, 7))\n",
    "librosa.display.specshow(chroma, sr=sample_rate, x_axis='time', y_axis='chroma', hop_length=512)\n",
    "plt.colorbar()\n",
    "plt.title('Chromagram')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 3D Spectrogram Visualization\n",
    "\n",
    "Let's create a 3D visualization of the spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute spectrogram\n",
    "D = np.abs(librosa.stft(audio_data, n_fft=2048, hop_length=512))\n",
    "D_db = librosa.amplitude_to_db(D, ref=np.max)\n",
    "\n",
    "# Create meshgrid for 3D plot\n",
    "times = librosa.times_like(D_db[0], sr=sample_rate, hop_length=512)\n",
    "freqs = librosa.fft_frequencies(sr=sample_rate, n_fft=2048)\n",
    "\n",
    "# Reduce dimensionality for better visualization\n",
    "step_t = 4  # Time step\n",
    "step_f = 4  # Frequency step\n",
    "times_reduced = times[::step_t]\n",
    "freqs_reduced = freqs[::step_f]\n",
    "D_db_reduced = D_db[::step_f, ::step_t]\n",
    "\n",
    "# Create meshgrid\n",
    "T, F = np.meshgrid(times_reduced, freqs_reduced)\n",
    "\n",
    "# Create 3D plot\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot surface\n",
    "surf = ax.plot_surface(T, F, D_db_reduced, cmap='viridis', alpha=0.8)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('Frequency (Hz)')\n",
    "ax.set_zlabel('Magnitude (dB)')\n",
    "ax.set_title('3D Spectrogram')\n",
    "\n",
    "# Add colorbar\n",
    "fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)\n",
    "\n",
    "# Set frequency axis to log scale\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim(20, sample_rate/2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Waveform Visualization with Plotly\n",
    "\n",
    "Let's create an interactive waveform visualization using Plotly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time array\n",
    "time = np.linspace(0, len(audio_data) / sample_rate, len(audio_data))\n",
    "\n",
    "# Create interactive waveform plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add waveform trace\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=time,\n",
    "    y=audio_data,\n",
    "    mode='lines',\n",
    "    name='Waveform',\n",
    "    line=dict(color='blue', width=1)\n",
    "))\n",
    "\n",
    "# Add RMS energy trace\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=rms_times,\n",
    "    y=rms,\n",
    "    mode='lines',\n",
    "    name='RMS Energy',\n",
    "    line=dict(color='red', width=2)\n",
    "))\n",
    "\n",
    "# Add mean line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, time[-1]],\n",
    "    y=[np.mean(audio_data), np.mean(audio_data)],\n",
    "    mode='lines',\n",
    "    name=f'Mean: {np.mean(audio_data):.4f}',\n",
    "    line=dict(color='green', width=1, dash='dash')\n",
    "))\n",
    "\n",
    "# Add max line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, time[-1]],\n",
    "    y=[np.max(np.abs(audio_data)), np.max(np.abs(audio_data))],\n",
    "    mode='lines',\n",
    "    name=f'Max: {np.max(np.abs(audio_data)):.4f}',\n",
    "    line=dict(color='orange', width=1, dash='dash')\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Interactive Audio Waveform',\n",
    "    xaxis_title='Time (s)',\n",
    "    yaxis_title='Amplitude',\n",
    "    hovermode='closest',\n",
    "    width=900,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "# Add range slider\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        rangeslider=dict(visible=True),\n",
    "        type='linear'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Spectrogram Visualization with Plotly\n",
    "\n",
    "Let's create an interactive spectrogram visualization using Plotly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive spectrogram plot\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=D_db,\n",
    "    x=times,\n",
    "    y=freqs,\n",
    "    colorscale='Viridis',\n",
    "    zmin=-80,\n",
    "    zmax=0\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Interactive Spectrogram',\n",
    "    xaxis_title='Time (s)',\n",
    "    yaxis_title='Frequency (Hz)',\n",
    "    yaxis_type='log',\n",
    "    width=900,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "# Add range slider\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        rangeslider=dict(visible=True),\n",
    "        type='linear'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Multi-panel Visualization\n",
    "\n",
    "Let's create a multi-panel visualization that combines different representations of the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-panel visualization\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 16))\n",
    "\n",
    "# Plot waveform\n",
    "librosa.display.waveshow(audio_data, sr=sample_rate, ax=axes[0])\n",
    "axes[0].set_title('Waveform')\n",
    "axes[0].set_xlabel('')\n",
    "axes[0].set_ylabel('Amplitude')\n",
    "\n",
    "# Plot spectrogram\n",
    "librosa.display.specshow(D_db, sr=sample_rate, x_axis='time', y_axis='log', hop_length=512, ax=axes[1])\n",
    "axes[1].set_title('Spectrogram')\n",
    "axes[1].set_xlabel('')\n",
    "fig.colorbar(axes[1].collections[0], ax=axes[1], format='%+2.0f dB')\n",
    "\n",
    "# Plot mel spectrogram\n",
    "librosa.display.specshow(mel_spec_db, sr=sample_rate, x_axis='time', y_axis='mel', hop_length=512, ax=axes[2])\n",
    "axes[2].set_title('Mel Spectrogram')\n",
    "axes[2].set_xlabel('')\n",
    "fig.colorbar(axes[2].collections[0], ax=axes[2], format='%+2.0f dB')\n",
    "\n",
    "# Plot chromagram\n",
    "librosa.display.specshow(chroma, sr=sample_rate, x_axis='time', y_axis='chroma', hop_length=512, ax=axes[3])\n",
    "axes[3].set_title('Chromagram')\n",
    "fig.colorbar(axes[3].collections[0], ax=axes[3])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored various techniques for visualizing audio data using the CTC-SpeechRefinement package. We've demonstrated different ways to represent audio signals visually, including waveforms, spectrograms, mel spectrograms, chromagrams, 3D spectrograms, and interactive visualizations. These visualization techniques can help in understanding the characteristics and patterns of audio signals, which is important for audio analysis, feature extraction, and speech recognition tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
